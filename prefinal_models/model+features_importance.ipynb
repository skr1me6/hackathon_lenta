{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWIElNQzQI7w"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dropout\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель на уже подготовленных данных. Тест=14 дней"
      ],
      "metadata": {
        "id": "poARix4ARdHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_new_no_dupl = pd.read_csv('/kaggle/working/final_wo_dupl.csv',index_col=0)"
      ],
      "metadata": {
        "id": "Me9btUGuQJ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seasons_unique = data_new_no_dupl.season.unique()\n",
        "season_dataframes = {}\n",
        "\n",
        "for season in seasons_unique:\n",
        "    season_dataframes[season] = data_new_no_dupl[data_new_no_dupl.season == season].copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "XKObMufCQJ_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# функцию потерь wape_loss\n",
        "def wape_loss(y_true, y_pred):\n",
        "    y_true_float = K.cast(y_true, dtype='float32')\n",
        "    error = K.abs(y_true_float - y_pred)\n",
        "    denominator = K.abs(y_true_float) + K.abs(y_pred)\n",
        "    return K.sum(error) / K.sum(denominator)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "lag_1_window = 7\n",
        "lag_2_window = 9\n",
        "seasons_unique = data_new.season.unique()\n",
        "season_loss_dct = {}"
      ],
      "metadata": {
        "id": "D5-g7L-cQKBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Создаем новый DataFrame для хранения лагов|\n",
        "for season in tqdm(season_dataframes.keys(), desc=\"Processing Season\"):\n",
        "    test_end_date = season_dataframes[season]['date'].max()\n",
        "    # Вычисляем начальную дату, отсчитывая от конца\n",
        "    test_start_date = test_end_date - timedelta(days=days_to_keep_in_test - 1)\n",
        "    # Фильтруем данные для тестовой выборки\n",
        "    data_new_test = season_dataframes[season][(season_dataframes[season]['date'] >= test_start_date) & (season_dataframes[season]['date'] <= test_end_date)]\n",
        "    # Фильтруем данные для тренировочной выборки (оставляем все, кроме тестовой части)\n",
        "    data_new_train = season_dataframes[season][season_dataframes[season]['date'] < test_start_date]\n",
        "    lag_df_train = pd.DataFrame()\n",
        "    # Перебираем уникальные пары магазин-товар\n",
        "    for store in tqdm(data_new_train.st_id.unique(), desc=\"Processing Stores Train\"):\n",
        "        for product in data_new_train.pr_sku_id.unique():\n",
        "            subset = data_new_train[(data_new_train['st_id'] == store) & (data_new_train['pr_sku_id'] == product)].copy()\n",
        "\n",
        "            # Фильтруем DataFrame для текущей пары магазин-товар\n",
        "            subset['lag_1_sales'] = subset['pr_sales_in_units'].shift(periods=1)\n",
        "            subset['lag_2_sales'] = subset['pr_sales_in_units'].shift(periods=2)\n",
        "            subset['lag_7_sales'] = subset['pr_sales_in_units'].shift(periods=7)\n",
        "            subset['lag_14_sales'] = subset['pr_sales_in_units'].shift(periods=14)\n",
        "            # Вычисляем скользящее среднее за 7 дней\n",
        "#             subset['rolling_mean_1'] = subset['pr_sales_in_units'].shift().rolling(window=1, min_periods=1).mean()\n",
        "            subset['rolling_mean_2'] = subset['pr_sales_in_units'].shift().rolling(window=2, min_periods=2).mean()\n",
        "            subset['rolling_mean_7'] = subset['pr_sales_in_units'].shift().rolling(window=7, min_periods=7).mean()\n",
        "            subset['rolling_mean_14'] = subset['pr_sales_in_units'].shift().rolling(window=14, min_periods=14).mean()\n",
        "\n",
        "            # Заполняем пропущенные значения скользящим средним\n",
        "#             subset['lag_1_sales'].fillna(rolling_mean_1, inplace=True)\n",
        "#             subset['lag_2_sales'].fillna(rolling_mean_2, inplace=True)\n",
        "#             subset['lag_7_sales'].fillna(rolling_mean_7, inplace=True)\n",
        "#             subset['lag_14_sales'].fillna(rolling_mean_14, inplace=True)\n",
        "\n",
        "            # Добавляем результаты в общий DataFrame\n",
        "            lag_df_train = pd.concat([lag_df_train, subset])\n",
        "            lag_df_train = lag_df_train.fillna(value=0)\n",
        "    # Создаем новый DataFrame для хранения лагов\n",
        "    lag_df_test = pd.DataFrame()\n",
        "\n",
        "    # Перебираем уникальные пары магазин-товар\n",
        "    for store in tqdm(data_new_test.st_id.unique(), desc=\"Processing Stores Test\"):\n",
        "        for product in data_new_test.pr_sku_id.unique():\n",
        "            # Фильтруем DataFrame для текущей пары магазин-товар\n",
        "            subset = data_new_test[(data_new_test['st_id'] == store) & (data_new_test['pr_sku_id'] == product)].copy()\n",
        "            # Создаем лаги\n",
        "            subset['lag_1_sales'] = subset['pr_sales_in_units'].shift(periods=1)\n",
        "            subset['lag_2_sales'] = subset['pr_sales_in_units'].shift(periods=2)\n",
        "            subset['lag_7_sales'] = subset['pr_sales_in_units'].shift(periods=7)\n",
        "            subset['lag_14_sales'] = subset['pr_sales_in_units'].shift(periods=14)\n",
        "            # Вычисляем скользящее среднее за 7 дней\n",
        "#             subset['rolling_mean_1'] = subset['pr_sales_in_units'].shift().rolling(window=1, min_periods=1).mean()\n",
        "            subset['rolling_mean_2'] = subset['pr_sales_in_units'].shift().rolling(window=2, min_periods=2).mean()\n",
        "            subset['rolling_mean_7'] = subset['pr_sales_in_units'].shift().rolling(window=7, min_periods=7).mean()\n",
        "            subset['rolling_mean_14'] = subset['pr_sales_in_units'].shift().rolling(window=14, min_periods=14).mean()\n",
        "            # Заполняем пропущенные значения скользящим средним\n",
        "            # Добавляем результаты в общий DataFrame\n",
        "            lag_df_test = pd.concat([lag_df_test, subset])\n",
        "            lag_df_test = lag_df_test.fillna(value=0)\n",
        "    lag_df_train.drop(['date'],axis=1,inplace=True)\n",
        "    lag_df_test.drop(['date'],axis=1,inplace=True)\n",
        "    features_lag_train = lag_df_train.drop('pr_sales_in_units',axis=1)\n",
        "    target_lag_train = lag_df_train.pr_sales_in_units\n",
        "    features_lag_test = lag_df_test.drop('pr_sales_in_units',axis=1)\n",
        "    target_lag_test = lag_df_test.pr_sales_in_units\n",
        "    # Теперь у вас есть словарь season_dataframes, где ключами являются уникальные сезоны,\n",
        "    # а значениями - соответствующие датафреймы.\n",
        "\n",
        "\n",
        "    # Создаем объект LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    features_lag_train['st_id'] = label_encoder.fit_transform(features_lag_train['st_id'])\n",
        "    features_lag_train['pr_sku_id'] = label_encoder.fit_transform(features_lag_train['pr_sku_id'])\n",
        "\n",
        "    features_lag_test['st_id'] = label_encoder.fit_transform(features_lag_test['st_id'])\n",
        "    features_lag_test['pr_sku_id'] = label_encoder.fit_transform(features_lag_test['pr_sku_id'])\n",
        "\n",
        "\n",
        "    # Перевод признаков и целевых значений в тензоры\n",
        "    features_train_tensor = tf.convert_to_tensor(features_lag_train.values, dtype=tf.float32)\n",
        "    target_train_tensor = tf.convert_to_tensor(target_lag_train.values, dtype=tf.int64)\n",
        "    # Перевод признаков и целевых значений в тензоры\n",
        "    features_test_tensor = tf.convert_to_tensor(features_lag_test.values, dtype=tf.float32)\n",
        "    target_test_tensor = tf.convert_to_tensor(target_lag_test.values, dtype=tf.int64)\n",
        "\n",
        "\n",
        "\n",
        "    # Создаем модель LSTM\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(features_train_tensor.shape[1], 1), return_sequences=True,kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(32, activation='tanh'))\n",
        "    model.add(Dropout(0.8))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "\n",
        "    # Компилируем модель с функцией потерь MSE\n",
        "    model.compile(optimizer=RMSprop(learning_rate=0.001), loss='mean_absolute_error', metrics=[wape_loss])\n",
        "\n",
        "\n",
        "\n",
        "    # Обучаем модель\n",
        "    model.fit(np.expand_dims(features_train_tensor, axis=-1), target_train_tensor, epochs=1, batch_size=32, verbose=1)\n",
        "\n",
        "    # Оцениваем модель на тестовой выборке\n",
        "    test_loss = model.evaluate(np.expand_dims(features_test_tensor, axis=-1), target_test_tensor)\n",
        "    season_loss_dct[season] = test_loss\n",
        "    # pickle file Сохранение модели\n",
        "    # model_filename = f\"{season}.pkl\"\n",
        "    # with open(model_filename, 'wb') as model_file:\n",
        "    #     pickle.dump(model, model_file)\n",
        "    # print(f\"Saved model to {model_filename}\")\n",
        "    print(f\"For season_{season} data frame test Loss (WAPE): {test_loss}\")"
      ],
      "metadata": {
        "id": "qd6vWYiMQtwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhEiyOk5Q9Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# График важности признаков"
      ],
      "metadata": {
        "id": "d0BG-TTRRlli"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8BuhHZpQ9QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results = []\n",
        "print('Computing LSTM feature importance...')\n",
        "\n",
        "#  базовые потери без перемешивания признаков\n",
        "baseline_loss = model.evaluate(np.expand_dims(features_test_tensor, axis=-1), target_test_tensor, verbose=0)\n",
        "results.append({'feature': 'BASELINE', 'loss': baseline_loss})\n",
        "\n",
        "#  имена столбцов признаков\n",
        "feature_names =  features_lag_test.columns\n",
        "\n",
        "for feature_column in tqdm(range(features_test_tensor.shape[1])):\n",
        "    #  значения признака для последующего восстановления\n",
        "    save_col = tf.identity(features_test_tensor[:, feature_column])\n",
        "\n",
        "    #  значения признака\n",
        "    shuffled_col = tf.random.shuffle(features_test_tensor[:, feature_column])\n",
        "\n",
        "    #  оригинальные значения перемешанными\n",
        "    features_test_tensor = tf.concat([features_test_tensor[:, :feature_column], shuffled_col[:, tf.newaxis], features_test_tensor[:, feature_column + 1:]], axis=-1)\n",
        "\n",
        "    #  потери модели после перемешивания признака\n",
        "    test_loss = model.evaluate(np.expand_dims(features_test_tensor, axis=-1), target_test_tensor, verbose=0)\n",
        "\n",
        "    # , что индекс в пределах допустимых значений\n",
        "    if feature_column < len(feature_names):\n",
        "        results.append({'feature': feature_names[feature_column], 'loss': test_loss})\n",
        "\n",
        "    #  исходные значения признака\n",
        "    features_test_tensor = tf.concat([features_test_tensor[:, :feature_column], save_col[:, tf.newaxis], features_test_tensor[:, feature_column + 1:]], axis=-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vlbXfyaCQ9SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print()\n",
        "df = pd.DataFrame(results)\n",
        "df = df.sort_values('loss')\n",
        "plt.figure(figsize=(5, 10))\n",
        "plt.barh(np.arange(len(df)), df.loss.apply(lambda x: float(x[0])))\n",
        "plt.yticks(np.arange(len(df)), df.feature.values)\n",
        "plt.title('LSTM Feature Importance', size=16)\n",
        "plt.ylim((-1, len(df)))\n",
        "plt.plot([baseline_loss, baseline_loss], [-1, len(df)], '--', color='orange',\n",
        "         label=f'Baseline Loss\\n{baseline_loss:.3f}')\n",
        "plt.xlabel(f'Test Loss with feature permuted', size=14)\n",
        "plt.ylabel('Feature', size=14)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HLgpBJCZRZXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}