{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip install catboost\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install numpy\n",
        "!pip install pickle"
      ],
      "metadata": {
        "id": "_jpZI_ZRElC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "import pickle\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ITSISHRHEeEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Рабочая директория на Kaggle\n",
        "base_directory = '/kaggle/working'\n",
        "\n",
        "# Перебираем каждый магазин и создаем папку с его названием\n",
        "for shop_name in unique_shops:\n",
        "    shop_directory = os.path.join(base_directory, shop_name)\n",
        "\n",
        "    # Проверяем, существует ли папка, и если нет, то создаем её\n",
        "    if not os.path.exists(shop_directory):\n",
        "        os.makedirs(shop_directory)\n",
        "\n",
        "    print(f'Создана папка: {shop_directory}')"
      ],
      "metadata": {
        "id": "ibSKtl3zFdO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_data = final_merged_filter.copy()"
      ],
      "metadata": {
        "id": "NCsomlWZL49n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_columns = ['st_id',\n",
        "              'pr_sku_id',\n",
        "              'pr_sales_in_units',\n",
        "              'holiday',\n",
        "              'year','cos_day',\n",
        "              'cos_month',\n",
        "              'cos_day_of_week',\n",
        "              'cos_season',\n",
        "              'before_holidays_n_days',\n",
        "              'after_holidays_n_days']\n"
      ],
      "metadata": {
        "id": "DQt-F2P_Lv1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_data = nn_data[nn_columns]"
      ],
      "metadata": {
        "id": "65D613xlL0hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_directory = '/kaggle/working'\n",
        "\n",
        "contents = os.listdir(base_directory)\n",
        "\n",
        "# Отфильтруем только папки\n",
        "folders = [item for item in contents if os.path.isdir(os.path.join(base_directory, item))]\n",
        "\n",
        "# Выводим список папок\n",
        "print(\"Список папок в рабочей директории:\")\n",
        "for folder in folders:\n",
        "    print(folder)\n"
      ],
      "metadata": {
        "id": "eQZE75mHE906"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wape(y_true: np.array, y_pred: np.array):\n",
        "    return np.sum(np.abs(y_true-y_pred))/np.sum(np.abs(y_true))\n",
        "\n",
        "# Словарь для хранения результатов для каждой модели\n",
        "results = {}\n",
        "mae_lst=[]"
      ],
      "metadata": {
        "id": "haR80zjSEeLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_shops = nn_data['st_id'].unique()\n",
        "unique_skus = nn_data['pr_sku_id'].unique()"
      ],
      "metadata": {
        "id": "e1zx1QO-Lp-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "\n",
        "# Цикл по каждому магазину\n",
        "for shop_id in tqdm(unique_shops, desc=\"Магазины\"):\n",
        "    # Фильтруем данные для текущего магазина\n",
        "    shop_data = nn_data[nn_data['st_id'] == shop_id]\n",
        "    # Создаем путь к директории для сохранения моделей, где shop_id - идентификатор магазина\n",
        "    model_dir = os.path.join(os.getcwd(), shop_id)\n",
        "    # Цикл по каждому товару в магазине\n",
        "    for sku_id in tqdm(unique_skus, desc=\"Товары\"):\n",
        "        # Фильтруем данные для текущего товара\n",
        "        subset = shop_data[shop_data['pr_sku_id'] == sku_id]\n",
        "\n",
        "        # Извлекаем признаки и целевую переменную для данной комбинации\n",
        "        features = subset.drop(['st_id', 'pr_sku_id', 'pr_sales_in_units'], axis=1)\n",
        "        target = subset['pr_sales_in_units']\n",
        "\n",
        "        # Пропускаем обработку, если количество данных меньше 7\n",
        "        if len(features) <= 6:\n",
        "            continue\n",
        "\n",
        "        # Логарифмическое преобразование целевой переменной\n",
        "        target_log = np.log1p(target)\n",
        "\n",
        "        # Создаем список моделей для ансамбля\n",
        "        models = [LinearRegression(), RandomForestRegressor(), GradientBoostingRegressor()]\n",
        "        # Создаем список моделей для ансамбля\n",
        "        models = [\n",
        "            ('linear', LinearRegression()),\n",
        "            ('random_forest', RandomForestRegressor()),\n",
        "            ('gradient_boosting', GradientBoostingRegressor())\n",
        "        ]\n",
        "\n",
        "        # Создаем ансамбль из моделей с помощью VotingRegressor\n",
        "        ensemble_model_1 = VotingRegressor(estimators=models)\n",
        "\n",
        "        # Массив для хранения предсказаний ансамбля\n",
        "        ensemble_predictions = np.zeros(len(target_log))\n",
        "\n",
        "        # Количество разбиений и TimeSeriesSplit\n",
        "        n_splits = 5\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "        # Цикл по каждому разбиению в кросс-валидации\n",
        "        for train_index, val_index in tscv.split(features):\n",
        "            X_train_fold, X_val_fold = features.iloc[train_index], features.iloc[val_index]\n",
        "            y_train_fold, y_val_fold = target_log.iloc[train_index], target_log.iloc[val_index]\n",
        "\n",
        "            # Обучаем ансамбль на тренировочных данных\n",
        "            ensemble_model_1.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "            # Предсказания на валидационных данных\n",
        "            val_predictions = ensemble_model_1.predict(X_val_fold)\n",
        "\n",
        "            # Сохраняем предсказания для данного разбиения\n",
        "            ensemble_predictions[val_index] = val_predictions\n",
        "\n",
        "        # Вычисляем качество ансамбля на валидационных данных\n",
        "        wape_first = wape(np.log1p(target), ensemble_predictions)\n",
        "        mae_first = mean_absolute_error(np.log1p(target), ensemble_predictions)\n",
        "        mae_lst.append(mae_first)\n",
        "        if wape_first > 1:\n",
        "            # Создаем экземпляры дополнительных моделей\n",
        "            xgb_model = XGBRegressor()\n",
        "            svr_model = SVR()\n",
        "            lr_model = LinearRegression()\n",
        "\n",
        "            # Создаем ансамбль из дополнительных моделей\n",
        "            ensemble_models = [\n",
        "                ('xgboost', xgb_model),\n",
        "                ('svr', svr_model),\n",
        "                ('linear_regression', lr_model)\n",
        "            ]\n",
        "\n",
        "            # Цикл по каждой модели в ансамбле\n",
        "            for model_name, model in ensemble_models:\n",
        "                # Создаем ансамбль только с текущей моделью\n",
        "                ensemble_model = VotingRegressor(estimators=[(model_name, model)])\n",
        "\n",
        "                # Массив для хранения предсказаний для каждой модели\n",
        "                predictions = np.zeros((len(target_log),))\n",
        "\n",
        "                # Цикл по каждому разбиению в кросс-валидации\n",
        "                for train_index, val_index in tscv.split(features):\n",
        "                    X_train_fold, X_val_fold = features.iloc[train_index], features.iloc[val_index]\n",
        "                    y_train_fold, y_val_fold = target_log.iloc[train_index], target_log.iloc[val_index]\n",
        "\n",
        "                    # Обучаем ансамбль на тренировочных данных\n",
        "                    ensemble_model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "                    # Предсказания на валидационных данных\n",
        "                    val_predictions = ensemble_model.predict(X_val_fold)\n",
        "\n",
        "                    # Сохраняем предсказания для данной модели и данного разбиения\n",
        "                    predictions[val_index] = val_predictions\n",
        "\n",
        "                # Вычисляем качество ансамбля на валидационных данных\n",
        "                wape_second = wape(np.log1p(target), predictions)\n",
        "\n",
        "                # Если второй WAPE лучше, сохраняем результаты\n",
        "                if wape_second < wape_first:\n",
        "                    results[(shop_id, sku_id)] = wape_second\n",
        "                    model_name = f\"{sku_id}.pkl\"\n",
        "                    model_path = os.path.join(model_dir, model_name)\n",
        "                    with open(model_path, 'wb') as model_file:\n",
        "                        pickle.dump(ensemble_model, model_file)\n",
        "                else:\n",
        "                    results[(shop_id, sku_id)] = wape_first\n",
        "                    model_name = f\"{sku_id}.pkl\"\n",
        "                    model_path = os.path.join(model_dir, model_name)\n",
        "                    with open(model_path, 'wb') as model_file:\n",
        "                        pickle.dump(ensemble_model_1, model_file)\n",
        "        else:\n",
        "            # Сохраняем результаты в словаре\n",
        "            results[(shop_id, sku_id)] = wape_first\n",
        "            model_name = f\"{sku_id}.pkl\"\n",
        "            model_path = os.path.join(model_dir, model_name)\n",
        "            with open(model_path, 'wb') as model_file:\n",
        "                pickle.dump(ensemble_model_1, model_file)\n",
        "\n",
        "# Распечатайте или используйте словарь с результатами по вашему усмотрению\n"
      ],
      "metadata": {
        "id": "smuim_qjEeNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "lst= []\n",
        "for i in results.values():\n",
        "    if i>1:\n",
        "        c+=1\n",
        "    else:\n",
        "        lst.append(i)"
      ],
      "metadata": {
        "id": "uNHzYa0AEsrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'mean MAE on test: {np.mean(mae_lst)}')"
      ],
      "metadata": {
        "id": "KuIUgVxuEstb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'mean WAPE on test (Without outliers): {np.mean(list(results.values())}')"
      ],
      "metadata": {
        "id": "F3Si2MyvEsxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlzbCTS5EszC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}